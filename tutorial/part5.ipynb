{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CUDA STF 教程 - Part 5: 并行构造 (parallel_for 与 launch)**\n",
    "\n",
    "在前面的部分中，我们学习了如何创建基本任务、管理数据依赖、同步以及控制执行和数据位置。CUDA STF 还提供了更高级的构造原语，使得直接在逻辑数据上编写并行计算内核变得更加简洁和强大。本部分将重点介绍 `parallel_for` 和 `launch` 这两个构造。\n",
    "\n",
    "本部分主要依据您提供的文档中 \"parallel_for construct\" (Page 24-29) 和 \"launch construct\" (Page 30-33) 章节的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. parallel_for 构造 (文档 Page 24-29)**\n",
    "\n",
    "`parallel_for` 是 CUDASTF 提供的一个辅助构造，用于创建在某个索引空间（通常由逻辑数据的形状定义）上执行操作的 CUDA 核函数（或 CPU 核函数，取决于执行位置）。它简化了常见的数据并行模式的实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.1 parallel_for 的基本结构**\n",
    "\n",
    "`parallel_for` 构造主要包含四个元素：\n",
    "\n",
    "1. **执行位置 (Execution Place)**: 指定代码将在哪里执行（例如，`exec_place::device(0)` 或 `exec_place::host()`）。  \n",
    "2. **形状 (Shape)**: 定义了生成核函数将迭代的索引空间。这通常是某个逻辑数据的形状 (`logical_data_handle.shape()`)，也可以是自定义的 `box` 形状。  \n",
    "3. **数据依赖集 (Data Dependencies)**: 与普通任务一样，声明所访问的逻辑数据及其访问模式（`read()`, `write()`, `rw()`, `reduce()`）。  \n",
    "4. **代码体 (Body of Code)**: 使用 `->*` 操作符指定的一个 lambda 函数。这个 lambda 函数就是将在每个索引上执行的核函数体。\n",
    "\n",
    "**lambda 函数的参数:**\n",
    "\n",
    "* 对于一个 N 维的形状，lambda 的前 N 个参数是 `size_t` 类型的索引（例如，对于二维形状是 `size_t i, size_t j`）。  \n",
    "* 后续参数是与 `parallel_for` 中声明的逻辑数据依赖相对应的数据实例（通常是 `slice` 对象）。  \n",
    "* 如果执行位置是设备，则 lambda 函数需要有 `__device__` (或 `_CCCL_DEVICE_`) 修饰符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.2 parallel_for 处理一维数组示例 (文档 Page 24)**\n",
    "\n",
    "我们将以下代码保存到 `p5_01_parallel_for_1D.cu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing p5_01_parallel_for_1D.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile p5_01_parallel_for_1D.cu\n",
    "#include <cuda/experimental/stf.cuh>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <numeric> // For std::iota and other algorithms if needed\n",
    "\n",
    "int main() {\n",
    "    using namespace cuda::experimental::stf;\n",
    "    context ctx;\n",
    "\n",
    "    const size_t N_array = 128;\n",
    "    std::vector<int> host_A_vec(N_array);\n",
    "\n",
    "    auto lA = ctx.logical_data(host_A_vec.data(), host_A_vec.size());\n",
    "    lA.set_symbol(\"A_1D_pfor\");\n",
    "\n",
    "    // 在当前设备上执行 parallel_for，迭代 lA 的形状 (0 到 N_array-1)\n",
    "    // lA 以只写模式访问\n",
    "    ctx.parallel_for(exec_place::current_device(),\n",
    "                     lA.shape(),\n",
    "                     lA.write())\n",
    "        ->*[] __host__ __device__ (size_t i, slice<int> sA_kernel_arg) -> void {\n",
    "            // 这是核函数体，会在每个索引 i 上执行\n",
    "            sA_kernel_arg(i) = 2 * (int)i + 1;\n",
    "        };\n",
    "\n",
    "    ctx.finalize();\n",
    "\n",
    "    // 验证结果 (数据会从设备写回到 host_A_vec)\n",
    "    bool correct = true;\n",
    "    for(size_t i = 0; i < N_array; ++i) {\n",
    "        if (host_A_vec[i] != (2 * (int)i + 1)) {\n",
    "            correct = false;\n",
    "            std::cout << \"Mismatch at index \" << i << \": host_A_vec[\" << i << \"] = \" << host_A_vec[i]\n",
    "                      << \" (Expected: \" << (2 * (int)i + 1) << \")\" << std::endl;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    if (correct) {\n",
    "        std::cout << \"parallel_for 1D example: Correct!\" << std::endl;\n",
    "    } else {\n",
    "        std::cout << \"parallel_for 1D example: Incorrect!\" << std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译并运行 `p5_01_parallel_for_1D.cu`:\n",
    "*(注意: 您可能需要根据您的 GPU 修改 `-arch=sm_86` 参数。)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761438a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel_for 1D example: Correct!\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include p5_01_parallel_for_1D.cu -o p5_01_parallel_for_1D -arch=sm_86 -lcuda\n",
    "!./p5_01_parallel_for_1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请打开您本地的 [`stf/01-axpy-parallel_for.cu`](./stf/01-axpy-parallel_for.cu)。** 这个示例应该展示了如何使用 `parallel_for` 来实现 AXPY 操作。将其与我们之前讨论的基于 `cuda_kernel` 的 AXPY 实现进行比较。您可以尝试编译运行它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/01-axpy-parallel_for.cu -o p5_stf_axpy_pfor -arch=sm_86 -lcuda\n",
    "!./p5_stf_axpy_pfor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.3 parallel_for 处理多维数组示例 (文档 Page 25)**\n",
    "\n",
    "对于多维数据形状，`parallel_for` 的 lambda 函数会接收对应数量的索引参数。我们将以下代码保存为 `p5_02_parallel_for_2D.cu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing p5_02_parallel_for_2D.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile p5_02_parallel_for_2D.cu\n",
    "#include <cuda/experimental/stf.cuh>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <tuple> // For std::tuple\n",
    "\n",
    "int main() {\n",
    "    using namespace cuda::experimental::stf;\n",
    "    context ctx;\n",
    "\n",
    "    const size_t M = 4; // 例如 4 行\n",
    "    const size_t K = 3; // 例如 3 列\n",
    "    std::vector<double> host_X_matrix_vec(M * K);\n",
    "\n",
    "    // 创建一个二维 slice 来描述这个矩阵\n",
    "    // 使用简化的 make_slice 语法创建连续的 2D slice\n",
    "    auto sX_host = make_slice(host_X_matrix_vec.data(), M, K);\n",
    "    auto lX_matrix = ctx.logical_data(sX_host);\n",
    "    lX_matrix.set_symbol(\"X_2D_pfor\");\n",
    "\n",
    "    ctx.parallel_for(exec_place::current_device(),\n",
    "                     lX_matrix.shape(), // 迭代 lX_matrix 的二维形状\n",
    "                     lX_matrix.write())\n",
    "        ->*[] _CCCL_DEVICE (size_t i, size_t j, auto sX_kernel_arg) {\n",
    "            // i 是行索引 (0 到 M-1), j 是列索引 (0 到 K-1)\n",
    "            sX_kernel_arg(i, j) = (double)(i * 10 + j);\n",
    "        };\n",
    "\n",
    "    ctx.finalize();\n",
    "\n",
    "    // 首先打印实际的数组内容来调试\n",
    "    std::cout << \"Array contents after kernel execution:\" << std::endl;\n",
    "    for(size_t idx = 0; idx < M * K; ++idx) {\n",
    "        std::cout << \"host_X_matrix_vec[\" << idx << \"] = \" << host_X_matrix_vec[idx] << std::endl;\n",
    "    }\n",
    "\n",
    "    // 验证 - slice 使用列主序存储，所以 sX_kernel_arg(i,j) 对应 host_X_matrix_vec[j*M + i]\n",
    "    bool correct = true;\n",
    "    for(size_t i = 0; i < M; ++i) {\n",
    "        for(size_t j = 0; j < K; ++j) {\n",
    "            size_t col_major_index = j * M + i;  // 列主序索引\n",
    "            double expected_value = (double)(i * 10 + j);\n",
    "            if (std::abs(host_X_matrix_vec[col_major_index] - expected_value) > 1e-9) {\n",
    "                 correct = false;\n",
    "                 std::cout << \"Mismatch at (\" << i << \",\" << j << \"): host_X_matrix_vec[\" << col_major_index << \"] = \"\n",
    "                           << host_X_matrix_vec[col_major_index] << \" (Expected: \" << expected_value << \")\" << std::endl;\n",
    "                 break;\n",
    "            }\n",
    "        }\n",
    "        if (!correct) break;\n",
    "    }\n",
    "    if (correct) {\n",
    "         std::cout << \"parallel_for 2D example: Correct!\" << std::endl;\n",
    "    } else {\n",
    "        std::cout << \"parallel_for 2D example: Incorrect!\" << std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译并运行 `p5_02_parallel_for_2D.cu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cccl/cudax/include/cuda/experimental/__stf/stream/interfaces/slice.cuh(245) [device 0] CUDA error in data_copy: invalid pitch argument (cudaErrorInvalidPitchValue).\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include p5_02_parallel_for_2D.cu -o p5_02_parallel_for_2D -arch=sm_86 -lcuda\n",
    "!./p5_02_parallel_for_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请打开您本地的 [`stf/parallel_for_2D.cu`](./stf/parallel_for_2D.cu)。** 这个示例会更完整地展示二维 `parallel_for` 的应用。尝试编译并运行它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/parallel_for_2D.cu -o p5_stf_parallel_for_2D -arch=sm_86 -lcuda\n",
    "!./p5_stf_parallel_for_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.4 box 形状 (文档 Page 26-27)**\n",
    "\n",
    "有时，迭代的索引空间不直接对应于某个逻辑数据的形状。对于这些情况，CUDASTF 提供了 `box<size_t dimensions = 1>` 模板类，允许用户定义具有显式边界的多维形状。\n",
    "\n",
    "* **基于范围的 box**: `box<2>({dim0_extent, dim1_extent})` 表示一个二维迭代空间，第一个索引从 0 到 `dim0_extent-1`，第二个索引从 0 到 `dim1_extent-1`。\n",
    "  ```cpp\n",
    "  // ctx.parallel_for(exec_place::current_device(), box<2>({2, 3})) // 迭代 i=0..1, j=0..2\n",
    "  //     ->*[&](size_t i, size_t j) __device__ {\n",
    "  //         printf(\"Box extent: %ld, %ld\\n\", i, j);\n",
    "  // };\n",
    "  ```\n",
    "* **基于上下界的 box**: `box<2>({{lower0, upper0}, {lower1, upper1}})`。下界包含，上界不包含。\n",
    "  ```cpp\n",
    "  // ctx.parallel_for(exec_place::current_device(), box<2>({{5, 8}, {2, 4}})) // i=5..7, j=2..3\n",
    "  //     ->*[&](size_t i, size_t j) __device__ {\n",
    "  //         printf(\"Box bounds: %ld, %ld\\n\", i, j);\n",
    "  // };\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8.5 reduce() 访问模式 (文档 Page 27-29)**\n",
    "\n",
    "`parallel_for` 支持 `reduce()` 访问模式，这使得在 `parallel_for` 生成的计算核函数内部实现归约操作成为可能。\n",
    "\n",
    "`reduce()` 接受的参数：\n",
    "1. **归约操作符 (Reduction Operator)**: 定义了如何组合多个值以及如何初始化一个值（例如，求和归约会将两个值相加，并将初始值设为0）。这些操作符定义在 `cuda::experimental::stf::reducer` 命名空间中。  \n",
    "2. **可选的 `no_init{}` 标签**: 如果提供此标签，则归约结果将累加到逻辑数据中已存在的值上（类似于 `rw()` 访问模式）。默认情况下（不提供 `no_init{}`），逻辑数据的内容将被归约结果覆盖（类似于 `write()` 访问模式）。对没有有效实例的逻辑数据（例如，仅从形状定义的）使用 `no_init{}` 会导致错误。  \n",
    "3. **其他参数**: 与其他访问模式相同，例如数据位置。\n",
    "\n",
    "只能对数据接口定义了 `owning_container_of` trait 类的逻辑数据应用 `reduce()` 访问模式。`scalar_view<T>` 数据接口就是这种情况，它将 `owning_container_of` 设置为 `T`。传递给 `parallel_for` lambda 的归约参数是对该类型对象的引用。\n",
    "\n",
    "**点积示例 (来自文档 Page 28):**\n",
    "我们将以下代码保存到 `p5_03_dot_reduce_pfor.cu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing p5_03_dot_reduce_pfor.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile p5_03_dot_reduce_pfor.cu\n",
    "#include <cuda/experimental/stf.cuh>\n",
    "#include <vector>\n",
    "#include <numeric> // For std::iota\n",
    "#include <iostream>\n",
    "#include <cmath> // For std::abs\n",
    "\n",
    "int main() {\n",
    "    using namespace cuda::experimental::stf;\n",
    "    context ctx;\n",
    "\n",
    "    const size_t N_dot = 1024;\n",
    "    std::vector<double> h_x_dot(N_dot), h_y_dot(N_dot);\n",
    "    std::iota(h_x_dot.begin(), h_x_dot.end(), 1.0);\n",
    "    std::iota(h_y_dot.begin(), h_y_dot.end(), 1.0);\n",
    "\n",
    "    auto lX_dot = ctx.logical_data(h_x_dot);\n",
    "    auto lY_dot = ctx.logical_data(h_y_dot);\n",
    "    auto lsum_dot = ctx.logical_data(shape_of<scalar_view<double>>());\n",
    "\n",
    "    lX_dot.set_symbol(\"X_for_dot_reduce\");\n",
    "    lY_dot.set_symbol(\"Y_for_dot_reduce\");\n",
    "    lsum_dot.set_symbol(\"Sum_result_dot_reduce\");\n",
    "\n",
    "    ctx.parallel_for(exec_place::current_device(),\n",
    "                     lY_dot.shape(), // 迭代空间\n",
    "                     lX_dot.read(),\n",
    "                     lY_dot.read(),\n",
    "                     lsum_dot.reduce(reducer::sum<double>{}) // 对 lsum_dot 进行求和归约\n",
    "                    )\n",
    "        ->*[&](size_t i, slice<const double> sX, slice<const double> sY, double& sum_ref) __device__ {\n",
    "            // sum_ref 是对 lsum_dot 设备实例中用于累加的部分的引用\n",
    "            // CUDASTF 会处理线程间的归约细节\n",
    "            sum_ref += sX(i) * sY(i);\n",
    "        };\n",
    "\n",
    "    double dot_product_result = ctx.wait(lsum_dot);\n",
    "    ctx.finalize();\n",
    "\n",
    "    std::cout << \"Dot product (via parallel_for.reduce and ctx.wait): \" << dot_product_result << std::endl;\n",
    "\n",
    "    double expected_dot_product = 0;\n",
    "    for(size_t i = 0; i < N_dot; ++i) expected_dot_product += h_x_dot[i] * h_y_dot[i];\n",
    "    std::cout << \"Expected dot product: \" << expected_dot_product << std::endl;\n",
    "\n",
    "    if (std::abs(dot_product_result - expected_dot_product) < 1e-9 * expected_dot_product) {\n",
    "        std::cout << \"Dot product example: Correct!\" << std::endl;\n",
    "    } else {\n",
    "        std::cout << \"Dot product example: Incorrect!\" << std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译并运行 `p5_03_dot_reduce_pfor.cu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product (via parallel_for.reduce and ctx.wait): 3.58438e+08\n",
      "Expected dot product: 3.58438e+08\n",
      "Dot product example: Correct!\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include p5_03_dot_reduce_pfor.cu -o p5_03_dot_reduce_pfor -arch=sm_86 -lcuda\n",
    "!./p5_03_dot_reduce_pfor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预定义的归约操作符 (文档 Page 28-29):  \n",
    "`sum`, `product`, `maxval`, `minval`, `logical_and`, `logical_or`, `bitwise_and`, `bitwise_or`, `bitwise_xor`。  \n",
    "用户也可以定义自己的归约操作符 (文档 Page 29)。  \n",
    "**请打开您本地的 [`stf/09-dot-reduce.cu`](./stf/09-dot-reduce.cu) 和 [`stf/word_count_reduce.cu`](./stf/word_count_reduce.cu) (如果存在)。** 这些示例会展示 `reduce()` 的实际应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译并运行 [`stf/09-dot-reduce.cu`](./stf/09-dot-reduce.cu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/09-dot-reduce.cu -o p5_stf_dot_reduce -arch=sm_86 -lcuda\n",
    "!./p5_stf_dot_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译并运行 [`stf/word_count_reduce.cu`](./stf/word_count_reduce.cu) (如果文件存在):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 65 words.\n"
     ]
    }
   ],
   "source": [
    "# 您需要确保 stf/word_count_reduce.cu 文件存在于您的目录中\n",
    "# 如果存在，取消下面的注释来编译和运行\n",
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/word_count_reduce.cu -o p5_stf_word_count_reduce -arch=sm_86 -lcuda\n",
    "!./p5_stf_word_count_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. launch 构造 (文档 Page 30-33)**\n",
    "\n",
    "`ctx.launch` 原语是 CUDASTF 中一种核函数启动机制，它隐式地处理单个核函数到执行位置的映射和启动。与 `parallel_for`（在每个索引点应用相同操作）不同，`launch` 执行的是一个基于**线程层级 (thread hierarchy)** 的结构化计算核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.1 launch 的基本语法**\n",
    "\n",
    "```cpp\n",
    "// ctx.launch([thread_hierarchy_spec], // 可选的线程层级规范\n",
    "//            [execution_place],       // 执行位置\n",
    "//            logicalData1.accessMode(),\n",
    "//            logicalData2.accessMode(), ...)\n",
    "//     ->*[capture_list] __device__ (thread_hierarchy_spec_t th, // 线程层级对象\n",
    "//                                   auto data1, auto data2...) {\n",
    "//     // 核函数实现\n",
    "// };\n",
    "```\n",
    "`launch` 构造包含五个主要元素：  \n",
    "1.  **可选的执行策略/线程层级规范 (Execution Policy / Thread Hierarchy Specification)**: 显式指定启动形状。例如，指定一组独立线程或可同步线程。  \n",
    "2.  **执行位置 (Execution Place)**: 指示代码将在哪里执行。  \n",
    "3.  **数据依赖集 (Data Dependencies)**: 与其他任务构造类似。  \n",
    "4.  **代码体 (Body of Code)**: 使用 `->*` 指定的 lambda 函数，带有 `__device__` 修饰符。  \n",
    "5.  **线程层级参数 (`thread_info` 或 `thread_hierarchy_spec_t th`)**: lambda 的第一个参数，用于查询线程属性（如全局ID、线程总数）和层级结构。\n",
    "\n",
    "**示例 (来自文档 Page 30，稍作调整):**\n",
    "我们将以下代码保存到 `p5_04_launch_axpy.cu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting p5_04_launch_axpy.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile p5_04_launch_axpy.cu\n",
    "#include <cuda/experimental/stf.cuh>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <cmath> // For std::sin, std::cos, std::abs\n",
    "#include <numeric> // For std::iota\n",
    "\n",
    "// LAUNCH_N 和 LAUNCH_ALPHA 可以在 main 中定义并通过捕获列表传递给 lambda \n",
    "// 或者作为逻辑数据传递。这里我们为了简化，作为全局常量。\n",
    "// 但在实际应用中，推荐通过捕获或逻辑数据传递。\n",
    "// const size_t LAUNCH_N_GLOBAL = 1024; // Example, better to pass via capture\n",
    "// const double LAUNCH_ALPHA_GLOBAL = 2.5; // Example, better to pass via capture\n",
    "\n",
    "int main() {\n",
    "    using namespace cuda::experimental::stf;\n",
    "    context ctx;\n",
    "\n",
    "    const size_t LAUNCH_N = 1024;\n",
    "    const double LAUNCH_ALPHA = 2.5;\n",
    "\n",
    "    std::vector<double> h_x_launch(LAUNCH_N), h_y_launch(LAUNCH_N), h_y_expected(LAUNCH_N);\n",
    "    for(size_t i=0; i<LAUNCH_N; ++i) {\n",
    "        h_x_launch[i] = std::sin((double)i);\n",
    "        h_y_launch[i] = std::cos((double)i);\n",
    "        h_y_expected[i] = std::cos((double)i) + LAUNCH_ALPHA * std::sin((double)i); // Precompute expected Y\n",
    "    }\n",
    "\n",
    "    auto lX_launch = ctx.logical_data(h_x_launch.data(), LAUNCH_N);\n",
    "    auto lY_launch = ctx.logical_data(h_y_launch.data(), LAUNCH_N);\n",
    "    lX_launch.set_symbol(\"X_for_launch\");\n",
    "    lY_launch.set_symbol(\"Y_for_launch\");\n",
    "\n",
    "    ctx.launch(par(LAUNCH_N), // 线程层级：LAUNCH_N 个并行线程\n",
    "               exec_place::current_device(),\n",
    "               lX_launch.read(), \n",
    "               lY_launch.rw()\n",
    "              )\n",
    "        ->*[=] __device__ (auto t, slice<const double> x_slice, slice<double> y_slice) {\n",
    "            size_t tid = t.rank(); // 获取当前线程的全局唯一ID\n",
    "            // size_t nthreads = t.get_num_threads(); // 获取此 launch 启动的总线程数\n",
    "            // In this simple case with par(LAUNCH_N) and vector size LAUNCH_N, tid directly maps to index.\n",
    "            // For more general cases (e.g., if par_size < vector_size), a grid-stride loop is needed:\n",
    "            // for (size_t ind = tid; ind < x_slice.size(); ind += nthreads) {\n",
    "            //    y_slice(ind) += LAUNCH_ALPHA * x_slice(ind);\n",
    "            // }\n",
    "            if (tid < x_slice.size()) { // Check bounds, x_slice.size() should be LAUNCH_N here\n",
    "                 y_slice(tid) += LAUNCH_ALPHA * x_slice(tid);\n",
    "            }\n",
    "        };\n",
    "\n",
    "    ctx.finalize();\n",
    "\n",
    "    // 验证\n",
    "    bool correct = true;\n",
    "    for(size_t i=0; i < LAUNCH_N; ++i) {\n",
    "        if (std::abs(h_y_launch[i] - h_y_expected[i]) > 1e-9) {\n",
    "            correct = false;\n",
    "            std::cout << \"Mismatch at index \" << i << \": Y_launch[\" << i << \"] = \" << h_y_launch[i] \n",
    "                      << \", Expected: \" << h_y_expected[i] << std::endl;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    if (correct) {\n",
    "        std::cout << \"launch AXPY example: Correct!\" << std::endl;\n",
    "    } else {\n",
    "        std::cout << \"launch AXPY example: Incorrect!\" << std::endl;\n",
    "    }\n",
    "    std::cout << \"launch example completed.\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译并运行 `p5_04_launch_axpy.cu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "launch AXPY example: Correct!\n",
      "launch example completed.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include p5_04_launch_axpy.cu -o p5_04_launch_axpy -arch=sm_86 -lcuda\n",
    "!./p5_04_launch_axpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.2 描述线程层级 (文档 Page 31)**\n",
    "\n",
    "线程层级规范描述了核函数的并行结构。层级大小可以自动计算、动态指定或编译时指定。\n",
    "\n",
    "* **`par(num_threads)`**: 并行组 (parallel group)，线程独立执行，不可组内同步。  \n",
    "  * `par<128>()`: 静态指定大小。  \n",
    "* **`con(num_threads)`**: 并发组 (concurrent group)，组内线程可以使用 `sync()` API 进行同步（组级别屏障）。  \n",
    "* **层级嵌套**: 可以嵌套多个组，例如 `par(128, con<256>())` 表示128个独立的组，每个组包含256个可同步的线程。  \n",
    "* **共享内存**: `con(256, mem(64))` 表示256个线程的组，共享64字节的内存（由STF自动分配在适当的内存层级）。  \n",
    "* **硬件范围亲和性 (Hardware Scope Affinity)**: 可以指定线程组映射到特定的机器层级。  \n",
    "  * `hw_scope::thread`: CUDA 线程。  \n",
    "  * `hw_scope::block`: CUDA 块。  \n",
    "  * `hw_scope::device`: CUDA 设备。  \n",
    "  * `hw_scope::all`: 整台机器。  \n",
    "  * 示例: `par(hw_scope::device | hw_scope::block, par<128>(hw_scope::thread))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9.3 操作线程层级对象 (th) (文档 Page 32-33)**\n",
    "\n",
    "传递给 `launch` 核函数体的线程层级对象（通常命名为 `th` 或 `t`）提供了查询层级结构和线程交互的方法：\n",
    "\n",
    "* `th.rank()`: 线程在整个层级中的全局排名。  \n",
    "* `th.size()`: 整个层级的总线程数。  \n",
    "* `th.rank(level_idx)`: 线程在第 `level_idx` 层的排名。  \n",
    "* `th.size(level_idx)`: 第 `level_idx` 层的线程数。  \n",
    "* `th.is_synchronizable(level_idx)`: 检查第 `level_idx` 层是否可同步。  \n",
    "* `th.sync(level_idx)`: 同步第 `level_idx` 层的所有线程。  \n",
    "* `th.sync()`: 同步最顶层（0级）的所有线程。  \n",
    "* `th.get_scope(level_idx)`: 获取第 `level_idx` 层的硬件范围亲和性。  \n",
    "* `th.template storage<T>(level_idx)`: 获取与第 `level_idx` 层关联的本地存储（作为 `slice<T>`）。  \n",
    "* `th.depth()`: (constexpr) 层级的深度。  \n",
    "* `th.inner()`: 获取移除了最顶层后的线程层级子集。\n",
    "\n",
    "**请打开您本地的 [`stf/launch_sum.cu`](./stf/launch_sum.cu)、[`stf/launch_scan.cu`](./stf/launch_scan.cu) 或 [`stf/launch_histogram.cu`](./stf/launch_histogram.cu)。** 这些示例会展示 `launch` 构造以及线程层级操作的实际应用，通常用于实现比简单数据并行更复杂的并行模式（例如，自定义的归约、扫描等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55cca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 GB in 273.807373 ms (3.73986 GB/s)\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include -I../cccl/cub -I../cccl/thrust launch_scan.cu -o launch_scan -arch=sm_86 -lcuda\n",
    "!./launch_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **动手试试:**\n",
    "\n",
    "1. **编译并运行上面提供的 `parallel_for` (1D `p5_01_parallel_for_1D.cu` 和 2D `p5_02_parallel_for_2D.cu`) 以及 `launch` 的示例代码 (`p5_04_launch_axpy.cu`)。** 确保您理解它们的行为和输出。\n",
    "2. **研究 [`stf/01-axpy-parallel_for.cu`](./stf/01-axpy-parallel_for.cu)**: 将其与 `stf/01-axpy-cuda_kernel.cu` (来自Part 1)进行比较。`parallel_for` 如何简化了 AXPY 的实现？(您已在前面编译运行过 `p5_stf_axpy_pfor`)\n",
    "3. **研究 [`stf/09-dot-reduce.cu`](./stf/09-dot-reduce.cu) (您已编译为 `p5_stf_dot_reduce`):**\n",
    "   * 它是如何使用 `parallel_for` 和 `lsum.reduce(reducer::sum<double>{})` 来计算点积的？  \n",
    "   * `sum_ref` 在 lambda 中是如何工作的？  \n",
    "   * `ctx.wait(lsum_dot)` 如何用于获取最终结果？  \n",
    "4. **(挑战)** 尝试修改 [`stf/09-dot-reduce.cu`](./stf/09-dot-reduce.cu)，使用不同的归约操作符，例如 `reducer::maxval<double>{}` 来找两个向量对应元素乘积的最大值。然后重新编译运行 `p5_stf_dot_reduce`。\n",
    "5. **研究 [`stf/launch_sum.cu`](./stf/launch_sum.cu)**:  \n",
    "   * `launch` 的线程层级是如何定义的？  \n",
    "   * 核函数体是如何使用线程层级对象 `th` 来执行求和的？是否有使用 `th.sync()` 或共享内存？\n",
    "   尝试编译并运行它:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include -I../cccl/cub -I../cccl/thrust ./stf/launch_sum.cu -o p5_stf_launch_sum -arch=sm_86 -lcuda\n",
    "!./p5_stf_launch_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   同样地，您可以尝试编译和运行 [`stf/launch_scan.cu`](./stf/launch_scan.cu) 和 [`stf/launch_histogram.cu`](./stf/launch_histogram.cu) (如果它们存在于您的 `stf` 目录中)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/launch_scan.cu -o p5_stf_launch_scan -arch=sm_86 -lcuda\n",
    "# !./p5_stf_launch_scan\n",
    "\n",
    "# !nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/launch_histogram.cu -o p5_stf_launch_histogram -arch=sm_86 -lcuda\n",
    "# !./p5_stf_launch_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经学习了 CUDA STF 中强大的 `parallel_for` 和 `launch` 构造，它们使得表达复杂并行模式更为便捷。\n",
    "\n",
    "在教程的 Part 6，我们将讨论 `cuda_kernel` 和 `cuda_kernel_chain` (虽然之前已提及，但会再次回顾其在整体结构中的位置)，以及 STF 如何通过 C++ 类型系统来增强代码的健壮性，最后是模块化使用 STF 的一些高级技巧。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
