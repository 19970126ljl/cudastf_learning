{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d275eee",
   "metadata": {},
   "source": [
    "## **CUDA STF 教程 - Part 4: 同步 (Synchronization) 与位置 (Places)**\n",
    "\n",
    "在前面的部分中，我们学习了如何定义逻辑数据和提交任务。现在，我们将探讨如何在 STF 中管理这些异步操作的完成，以及如何控制任务的执行位置和数据的存放位置。\n",
    "\n",
    "本部分主要依据您提供的文档中 \"Synchronization\" (Page 17-18) 和 \"Places\" (Page 18-24) 章节的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8463cc4",
   "metadata": {},
   "source": [
    "### **6. 同步 (Synchronization) (文档 Page 17-18)**\n",
    "\n",
    "如前所述，提交给 STF 上下文的任务体（lambda 函数）通常是立即在主机上执行的，其作用是将异步工作（如 CUDA 核函数）提交到 CUDA 流中。CUDASTF 确保在任务体内的操作可以一致地访问指定的数据，并遵循请求的访问模式。\n",
    "\n",
    "由于任务并行执行的异步性，必须确保所有操作都正确调度和执行。因为 CUDASTF 透明地处理数据管理（分配、传输等），可能存在一些用户未显式提交的悬而未决的异步操作。因此，**仅使用原生的 CUDA 同步操作（如 `cudaStreamSynchronize()` 或 `cudaDeviceSynchronize()`）是不够的**，因为它们不知道 CUDASTF 的内部状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c8fa1",
   "metadata": {},
   "source": [
    "#### **6.1 ctx.submit() 和 ctx.finalize()**\n",
    "\n",
    "* **`ctx.submit()`**:  \n",
    "  * 启动序列中所有异步任务的提交过程。  \n",
    "  * 通常，创建任务和调用 `ctx.finalize()` 就足够了。  \n",
    "  * 手动调用 `ctx.submit()` 在以下情况可能有用：  \n",
    "    1. 允许在提交和同步之间在 CPU（或其他 GPU）上执行额外的无关工作。  \n",
    "    2. 当需要两个上下文并发运行时，使用 `ctx1.submit(); ctx2.submit(); ctx1.finalize(); ctx2.finalize();` 可以实现此目标（而直接调用 `ctx1.finalize(); ctx2.finalize();` 会等待第一个任务完成才开始第二个）。  \n",
    "* **`ctx.finalize()`**:  \n",
    "  * 等待上下文中所有未完成的异步操作（包括任务、传输等）结束。  \n",
    "  * 如果用户代码未在此之前调用 `ctx.submit()`，`finalize()` 会自动调用它。  \n",
    "  * 这是确保所有 STF 管理的工作都已完成的主要机制。\n",
    "\n",
    "**示例 (来自文档 Page 17):**\n",
    "```cpp\n",
    "#include <cuda/experimental/stf.cuh>  \n",
    "// ... 其他必要的 includes ...\n",
    "\n",
    "int main() {  \n",
    "    using namespace cuda::experimental::stf;  \n",
    "    context ctx;\n",
    "\n",
    "    // ... 定义逻辑数据 lX, lY ...  \n",
    "    // ... 提交任务 ctx.task(lX.read(), lY.rw())->*[]{ ... }; ...\n",
    "\n",
    "    // 可选：提交所有挂起的任务，但不立即等待它们完成  \n",
    "    ctx.submit();\n",
    "\n",
    "    // 此时可以在 CPU 上执行一些与 STF 任务无关的工作  \n",
    "    // ... Unrelated CPU-based code might go here...\n",
    "\n",
    "    // 等待 STF 上下文中所有操作完成  \n",
    "    ctx.finalize();\n",
    "\n",
    "    return 0;  \n",
    "}\n",
    "```\n",
    "在您提供的 `stf/` 示例中，通常 `main` 函数结束时，`scheduler` 或 `context` 对象的析构函数会隐式地处理 `finalize` 的逻辑，或者 `scheduler.execute(...)` 调用本身就是阻塞的，直到该批次任务完成。对于需要显式控制的复杂场景，`submit()` 和 `finalize()` 提供了更细致的控制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a46c07",
   "metadata": {},
   "source": [
    "#### **6.2 任务栅栏 (ctx.task_fence())**\n",
    "\n",
    "这是一种等待所有挂起操作（任务、传输等）完成的异步栅栏机制。\n",
    "\n",
    "```cpp\n",
    "cudaStream_t stream_for_fence = ctx.task_fence();  \n",
    "// 现在，任何提交到 stream_for_fence 的后续 CUDA 工作  \n",
    "// 都会在其之前的 STF 任务完成后才开始。  \n",
    "// 如果需要主机等待，则可以同步这个特定的流：  \n",
    "cudaStreamSynchronize(stream_for_fence);  \n",
    "```\n",
    "`ctx.task_fence()` 返回一个 CUDA 流。STF 保证所有在调用 `task_fence()` 之前提交到上下文的任务和数据操作，都会在这个返回的流上的任何后续操作开始之前完成。这允许您将 STF 工作流与其他基于 CUDA 流的异步操作同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10699b33",
   "metadata": {},
   "source": [
    "#### **6.3 `ctx.wait(logical_data_handle)`**\n",
    "\n",
    "这是一种**阻塞调用**，用于等待特定的逻辑数据准备就绪，并返回其内容。返回值的类型由逻辑数据接口的 `owning_container_of<interface>` trait 类定义。\n",
    "\n",
    "* 此方法通常与 `reduce()` 访问模式结合使用，以实现动态控制流（例如，根据归约结果决定下一步操作）。\n",
    "* 不能在没有重载此 trait 类的接口的逻辑数据上调用 `wait()`。\n",
    "\n",
    "**示例 (参考文档 Page 28 的点积示例):**\n",
    "下面的代码演示了如何使用 `ctx.wait()` 来获取一个标量逻辑数据的计算结果。我们将代码写入 `p4_01_ctx_wait.cu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "439f6d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting p4_01_ctx_wait.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile p4_01_ctx_wait.cu\n",
    "#include <cuda/experimental/stf.cuh>\n",
    "#include <vector>\n",
    "#include <numeric> // for std::iota\n",
    "#include <iostream>\n",
    "#include <cmath> // for std::abs\n",
    "\n",
    "// 简化版的核函数，用于并行计算元素乘积并累加到全局和\n",
    "template<typename T>\n",
    "__global__ void accumulate_product(cuda::experimental::stf::slice<const T> x, cuda::experimental::stf::slice<const T> y, T* global_sum_output) {\n",
    "    extern __shared__ T sdata[];\n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    T local_sum = 0;\n",
    "    if (gid < x.size()) { // 确保不越界\n",
    "        local_sum = x(gid) * y(gid);\n",
    "    }\n",
    "    sdata[tid] = local_sum;\n",
    "    __syncthreads();\n",
    "\n",
    "    // Shared memory reduction (simplified for one block, assumes blockDim.x is power of 2)\n",
    "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
    "        if (tid < s) {\n",
    "            sdata[tid] += sdata[tid + s];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (tid == 0) {\n",
    "        atomicAdd(global_sum_output, sdata[0]);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    using namespace cuda::experimental::stf;\n",
    "    context ctx;\n",
    "\n",
    "    const size_t N = 1024;\n",
    "    std::vector<double> h_x(N), h_y(N);\n",
    "    std::iota(h_x.begin(), h_x.end(), 1.0); // 1.0, 2.0, ..., N\n",
    "    std::iota(h_y.begin(), h_y.end(), 1.0); // 1.0, 2.0, ..., N\n",
    "\n",
    "    auto lX = ctx.logical_data(&h_x[0], {N});\n",
    "    auto lY = ctx.logical_data(&h_y[0], {N});\n",
    "    auto lsum = ctx.logical_data(shape_of<scalar_view<double>>()); // Create scalar logical data for sum\n",
    "\n",
    "    lX.set_symbol(\"X_dot\");\n",
    "    lY.set_symbol(\"Y_dot\");\n",
    "    lsum.set_symbol(\"Sum_dot\");\n",
    "\n",
    "    // Task to compute dot product and store in lsum\n",
    "    ctx.task(exec_place::current_device(),\n",
    "             lX.read(),\n",
    "             lY.read(),\n",
    "             lsum.write() // Initialize lsum (or use reduce with no_init_tag for first access)\n",
    "            )\n",
    "        ->*[&](cudaStream_t s, slice<const double> sX, slice<const double> sY, scalar_view<double> sSum_scalar) {\n",
    "            cudaMemsetAsync(sSum_scalar.addr, 0, sizeof(double), s); // Initialize sum to 0 on device\n",
    "\n",
    "            dim3 threads_per_block(256);\n",
    "            dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x);\n",
    "            size_t shared_mem_size = threads_per_block.x * sizeof(double);\n",
    "\n",
    "            std::cout << \"Submitting accumulate_product kernel...\" << std::endl;\n",
    "            accumulate_product<<<num_blocks, threads_per_block, shared_mem_size, s>>>(sX, sY, sSum_scalar.addr);\n",
    "        };\n",
    "\n",
    "    // Block and wait for lsum's computation to complete and get its value\n",
    "    std::cout << \"Calling ctx.wait(lsum)...\" << std::endl;\n",
    "    double result = ctx.wait(lsum); // Host waits here until lsum is ready\n",
    "\n",
    "    // finalize() ensures all other operations (like potential write-backs for lX, lY if they were modified) are also complete.\n",
    "    // In this specific case, since lX and lY are read-only for the task, and lsum's value is already fetched by wait(), \n",
    "    // finalize() might not do much more for these specific handles but is good practice for overall context cleanup.\n",
    "    ctx.finalize(); \n",
    "\n",
    "    std::cout << \"Dot product result (obtained via ctx.wait): \" << result << std::endl;\n",
    "\n",
    "    double expected_sum = 0;\n",
    "    for(size_t i = 0; i < N; ++i) expected_sum += h_x[i] * h_y[i];\n",
    "    std::cout << \"Expected dot product: \" << expected_sum << std::endl;\n",
    "\n",
    "    if (std::abs(result - expected_sum) < 1e-5) {\n",
    "        std::cout << \"Result is correct!\" << std::endl;\n",
    "    } else {\n",
    "        std::cout << \"Result is INCORRECT! Difference: \" << std::abs(result - expected_sum) << std::endl;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b4eea",
   "metadata": {},
   "source": [
    "编译并运行 `p4_01_ctx_wait.cu`:\n",
    "*(注意: 您可能需要根据您的 GPU 修改 `-arch=sm_86` 参数。)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4600f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting accumulate_product kernel...\n",
      "Calling ctx.wait(lsum)...\n",
      "Dot product result (obtained via ctx.wait): 3.58438e+08\n",
      "Expected dot product: 3.58438e+08\n",
      "Result is correct!\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include p4_01_ctx_wait.cu -o p4_01_ctx_wait -arch=sm_86 -lcuda\n",
    "!./p4_01_ctx_wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112b606",
   "metadata": {},
   "source": [
    "**请打开您本地的 [`stf/09-dot-reduce.cu`](./stf/09-dot-reduce.cu)。** 这个示例应该更规范地展示了如何使用 `parallel_for` 和 `reduce()` 访问模式来计算点积，并可能使用 `ctx.wait()` 来获取最终的标量结果。您可以尝试编译并运行它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dfa6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/09-dot-reduce.cu -o p4_dot_reduce -arch=sm_86 -lcuda\n",
    "!./p4_dot_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd0d52",
   "metadata": {},
   "source": [
    "### **7. 位置 (Places) (文档 Page 18-24)**\n",
    "\n",
    "为了帮助用户管理数据和执行的亲和性 (affinity)，CUDASTF 提供了 **位置 (place)** 的概念。位置可以表示：\n",
    "\n",
    "* **执行位置 (Execution Places)**: 决定代码在哪里执行。  \n",
    "* **数据位置 (Data Places)**: 指定数据在机器非均匀内存中的位置。\n",
    "\n",
    "CUDASTF 的目标之一是默认确保数据根据执行位置进行高效放置，同时也为用户提供了在必要时轻松自定义放置的选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4b346",
   "metadata": {},
   "source": [
    "#### **7.1 执行位置 (Execution Places) (文档 Page 18-19)**\n",
    "\n",
    "任务的构造函数（或 `task()`, `cuda_kernel()` 等方法的第一个参数）允许选择一个执行位置。\n",
    "\n",
    "* `exec_place::current_device()`: (默认) 在当前活动的 CUDA 设备上运行。  \n",
    "* `exec_place::device(int device_id)`: 在指定的 `device_id` 的 CUDA 设备上运行。  \n",
    "* `exec_place::host()`: 在主机 CPU 上运行。\n",
    "\n",
    "重要说明：  \n",
    "无论执行位置如何，任务体（lambda 函数）本身是在主机上执行的 CPU 代码，其目的是异步地启动计算。\n",
    "\n",
    "* 当使用 `exec_place::device(id)` 时，CUDASTF 会在任务开始时自动将当前 CUDA 设备设置为 `id`，并在任务结束时恢复之前的设备。  \n",
    "* `exec_place::host()` 不会影响当前 CUDA 设备。  \n",
    "* 对于 `exec_place::host()`，如果使用 `ctx.task(exec_place::host(), ...)`，lambda 会接收一个 `cudaStream_t`。如前所述，用户需要小心处理同步，并且这种方式与 `graph_ctx` 不兼容。**推荐使用 `ctx.host_launch(...)` 来执行主机任务**，因为它通过 CUDA 回调机制维护了更好的异步语义，并与所有后端兼容。\n",
    "\n",
    "**示例 (来自文档 Page 19，稍作修改以便运行):**\n",
    "我们将以下代码保存为 `p4_02_exec_places.cu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de8d48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting p4_02_exec_places.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile p4_02_exec_places.cu\n",
    "#include <cuda/experimental/stf.cuh>\n",
    "#include <cassert>\n",
    "#include <iostream>\n",
    "#include <vector> // Required for STF logical_data from host data usually\n",
    "\n",
    "// 假设的简单核函数\n",
    "__global__ void inc_kernel(cuda::experimental::stf::slice<int> sX_slice) {\n",
    "    if (threadIdx.x == 0 && blockIdx.x == 0) {\n",
    "        sX_slice(0) += 1;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    using namespace cuda::experimental::stf;\n",
    "    context ctx;\n",
    "\n",
    "    int host_X_val_arr[1] = {42}; // Use an array for STF logical_data\n",
    "    auto lX = ctx.logical_data(host_X_val_arr); // STF manages this array now\n",
    "    lX.set_symbol(\"X_scalar\");\n",
    "\n",
    "    int num_devices = 0;\n",
    "    cudaError_t err = cudaGetDeviceCount(&num_devices);\n",
    "    if (err != cudaSuccess) {\n",
    "        std::cerr << \"cudaGetDeviceCount failed: \" << cudaGetErrorString(err) << std::endl;\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    if (num_devices < 1) {\n",
    "        std::cout << \"Requires at least 1 CUDA device, found \" << num_devices << \". Skipping device tasks.\" << std::endl;\n",
    "    } else {\n",
    "        std::cout << \"Found \" << num_devices << \" CUDA device(s). Proceeding with device tasks.\" << std::endl;\n",
    "        // 任务1: 在设备0上执行\n",
    "        ctx.task(exec_place::device(0), lX.rw())\n",
    "            ->*[&](cudaStream_t stream, slice<int> sX_kernel_arg) {\n",
    "                int current_device;\n",
    "                cudaGetDevice(&current_device);\n",
    "                std::cout << \"Task 1 (exec_place::device(0)): Current CUDA device is \" << current_device << \", incrementing X. Stream: \" << stream << std::endl;\n",
    "                inc_kernel<<<1, 1, 0, stream>>>(sX_kernel_arg);\n",
    "        };\n",
    "\n",
    "        if (num_devices > 1) {\n",
    "            // 任务2: 在设备1上执行 (如果存在)\n",
    "            ctx.task(exec_place::device(1), lX.rw())\n",
    "                ->*[&](cudaStream_t stream, slice<int> sX_kernel_arg) {\n",
    "                    int current_device;\n",
    "                    cudaGetDevice(&current_device);\n",
    "                    std::cout << \"Task 2 (exec_place::device(1)): Current CUDA device is \" << current_device << \", incrementing X. Stream: \" << stream << std::endl;\n",
    "                    inc_kernel<<<1, 1, 0, stream>>>(sX_kernel_arg);\n",
    "            };\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // 任务3: 在主机上执行，读取 lX (使用 host_launch)\n",
    "    ctx.host_launch(lX.read()) // exec_place::host() is implicit and optional for host_launch\n",
    "        .set_symbol(\"HostReadTask\")\n",
    "        ->*[&](slice<const int> sX_host_arg) { // lambda 参数是主机上的数据实例\n",
    "            std::cout << \"Host Task: reading X.\" << std::endl;\n",
    "            int increments = 0;\n",
    "            if (num_devices >= 1) increments++; // From Task 1\n",
    "            if (num_devices > 1) increments++;  // From Task 2\n",
    "            int expected_val = 42 + increments;\n",
    "            std::cout << \"Host Task: X(0) = \" << sX_host_arg(0) << \", Expected = \" << expected_val << std::endl;\n",
    "            assert(sX_host_arg(0) == expected_val);\n",
    "    };\n",
    "\n",
    "    ctx.finalize();\n",
    "    std::cout << \"Finalized. Original host_X_val_arr[0] after STF (due to write-back): \" << host_X_val_arr[0] << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbbe3d",
   "metadata": {},
   "source": [
    "编译并运行 `p4_02_exec_places.cu`。观察输出，特别是当您有多个GPU时任务的调度情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3c81ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA device(s). Proceeding with device tasks.\n",
      "Task 1 (exec_place::device(0)): Current CUDA device is 0, incrementing X. Stream: 0x55555601ff40\n",
      "Host Task: reading X.\n",
      "Host Task: X(0) = 43, Expected = 43\n",
      "Finalized. Original host_X_val_arr[0] after STF (due to write-back): 43\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include p4_02_exec_places.cu -o p4_02_exec_places -arch=sm_86 -lcuda\n",
    "!./p4_02_exec_places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1bdae",
   "metadata": {},
   "source": [
    "请打开您本地的 [`stf/explicit_data_places.cu`](./stf/explicit_data_places.cu) 或多GPU示例 [`stf/heat_mgpu.cu`](./stf/heat_mgpu.cu) / [`stf/fdtd_mgpu.cu`](./stf/fdtd_mgpu.cu)。\n",
    "这些示例会清晰地展示如何使用 `exec_place::device(id)` 来将计算任务分配到不同的 GPU 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b676c77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.000000 ms\n"
     ]
    }
   ],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/heat_mgpu.cu -o heat_mgpu -arch=sm_86 -lcuda\n",
    "!./heat_mgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682c188",
   "metadata": {},
   "source": [
    "#### **7.2 数据位置 (Data Places) (文档 Page 19-21)**\n",
    "\n",
    "默认情况下，逻辑数据与其当前处理它的任务的**执行位置的仿射数据位置 (affine data place)** 相关联。\n",
    "\n",
    "* 在设备上启动的任务，其数据默认加载到该设备的全局内存中。  \n",
    "* 在主机上执行的任务，其数据默认访问主机内存 (RAM)。\n",
    "\n",
    "您可以在声明数据依赖时，为逻辑数据显式指定一个数据位置：  \n",
    "`logical_data_handle.accessMode(data_place_specifier)`\n",
    "\n",
    "* `data_place::affine()`: (默认) 将数据定位在与执行位置仿射的数据位置。  \n",
    "* `data_place::managed()`: 使用统一内存 (managed memory)。  \n",
    "* `data_place::device(int device_id)`: 将数据放在指定 `device_id` 的 CUDA 设备的内存中（这可能与当前设备或任务的执行设备不同）。  \n",
    "* `data_place::host()`: 将数据放在主机内存中。\n",
    "\n",
    "**示例 (来自文档 Page 20, 概念性片段):**\n",
    "```cpp\n",
    "// context ctx;  \n",
    "// auto lA = ctx.logical_data(...);\n",
    "\n",
    "// 任务在 device 0 执行，数据 lA 也默认在 device 0 的内存中  \n",
    "// ctx.task(exec_place::device(0), lA.rw()) ->* ...\n",
    "\n",
    "// 等同于显式指定仿射数据位置  \n",
    "// ctx.task(exec_place::device(0), lA.rw(data_place::affine())) ->* ...  \n",
    "// ctx.task(exec_place::device(0), lA.rw(data_place::device(0))) ->* ...\n",
    "\n",
    "// 覆盖亲和性：任务在 device 0 执行，但访问位于主机内存的 lA 实例  \n",
    "// (假设系统支持从设备访问主机内存，例如通过统一虚拟内存 UVM)\n",
    "// ctx.task(exec_place::device(0), lA.rw(data_place::host())) ->* ...\n",
    "\n",
    "// 任务在主机执行，但访问位于 device 0 内存的 lA 实例  \n",
    "// (假设系统支持从主机访问设备内存)\n",
    "// ctx.task(exec_place::host(), lA.rw(data_place::device(0))) ->* ... \n",
    "// (更推荐: ctx.host_launch(lA.read(data_place::device(0))) ->* ... 对于主机任务访问特定设备数据)\n",
    "```\n",
    "覆盖数据亲和性在某些情况下可能有利，例如当任务仅稀疏访问大块逻辑数据时，可以避免传输大量数据（CUDA 统一内存的分页系统会自动调入实际使用的部分）。然而，这依赖于系统硬件（如 NVLINK、UVM）和操作系统（例如 WSL 对从 CUDA 核函数访问主机内存的支持可能有限且性能较低）。\n",
    "\n",
    "**请再次查看 [`stf/explicit_data_places.cu`](./stf/explicit_data_places.cu)。** 这个示例应该专门演示了如何显式控制数据位置，以及它与执行位置的交互。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114ab42",
   "metadata": {},
   "source": [
    "#### **7.3 位置网格 (Grid of Places) 与分区策略 (文档 Page 21-24)**\n",
    "\n",
    "这部分内容较为高级，主要用于多 GPU 或更复杂的分布式内存场景，允许将多个位置组织成网格，并定义数据如何在这些位置之间分区。\n",
    "\n",
    "* **`exec_place_grid`**: 描述执行位置的网格。可以从一个位置向量创建，或定义为多维网格。  \n",
    "  * `exec_place::all_devices()`: 一个创建包含所有可用设备网格的辅助函数。  \n",
    "* **分区策略 (Partitioning policies)**: 定义数据如何在网格中的不同位置分派，或者并行循环的索引空间如何在位置间分布。  \n",
    "  * `tiled_partition<TILE_SIZE>`: 使用瓦片式布局分派。  \n",
    "  * `blocked_partition`: 使用块式布局分派，网格中的每个条目大致接收形状的相同连续部分。\n",
    "\n",
    "这些高级功能对于编写高度可扩展的多 GPU 应用非常重要。如果您计划进行此类开发，建议详细阅读文档的这部分内容，并研究 [`stf/heat_mgpu.cu`](./stf/heat_mgpu.cu) 或 [`stf/linear_algebra/06-pdgemm.cu`](./stf/linear_algebra/06-pdgemm.cu) (分布式 GEMM) 等示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a8136",
   "metadata": {},
   "source": [
    "### **动手试试:**\n",
    "\n",
    "1. **编译并运行上面修改过的 `p4_02_exec_places.cu` 示例代码。** 如果您有多个 GPU，观察任务是否确实在不同的设备上调度（可以通过 `nvidia-smi` 或在任务体中打印设备 ID 来粗略判断。注意 `cudaGetDevice()` 在 `exec_place::device(N)` 的 task lambda 中会返回 `N`）。\n",
    "2. **研究 [`stf/explicit_data_places.cu`](./stf/explicit_data_places.cu)**: 理解它是如何显式地将数据放置在与执行位置不同的地方，以及这可能带来的影响。尝试编译并运行它:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/explicit_data_places.cu -o p4_explicit_data_places -arch=sm_86 -lcuda\n",
    "!./p4_explicit_data_places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdee3a",
   "metadata": {},
   "source": [
    "3. **(可选，如果有多 GPU 环境)** 尝试理解 [`stf/heat_mgpu.cu`](./stf/heat_mgpu.cu) 是如何使用 `exec_place::device(id)` 来划分工作的。注意查找是否有显式的拷贝任务（例如 `stf_ctx.copy_task(...)`）用于在不同 GPU 的数据实例之间同步数据（例如 halo 交换）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66069a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 23.979008 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 编译 heat_mgpu (如果需要，调整路径和目标名)\n",
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include ./stf/heat_mgpu.cu -o heat_mgpu -arch=sm_86 -lcuda \n",
    "!./heat_mgpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde574b",
   "metadata": {},
   "source": [
    "我们现在对 STF 中的同步机制和位置管理有了更深入的了解。这些概念对于编写正确且高效的异构并行程序至关重要。\n",
    "\n",
    "在教程的 Part 5，我们将开始学习 STF 提供的高级并行构造原语：`parallel_for` 和 `launch`，它们使得在逻辑数据上直接编写并行计算变得更加简洁。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
