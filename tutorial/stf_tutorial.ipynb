{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXATwZ8VxZG7"
   },
   "source": [
    "# CUDA STF Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyB9gzUjSGKc"
   },
   "source": [
    "### Enabled GPU in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynQEmQTgW_hs"
   },
   "source": [
    "### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2125,
     "status": "ok",
     "timestamp": 1741281010875,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "8GOdF9A-XDcf",
    "outputId": "5cfc8e1e-0ff6-4f57-a5a8-809c0dd348d2"
   },
   "outputs": [],
   "source": [
    "!source ../env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pfFUL-maxh4"
   },
   "source": [
    "### Check if GPU is running or not, run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1741280001632,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "cIWeteLFbMdM",
    "outputId": "d118ef23-a2f0-4c68-ec17-972e19947bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 22 12:39:18 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.05              Driver Version: 535.86.05    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 72%   67C    P2             233W / 350W |  18966MiB / 24576MiB |     83%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 78%   75C    P2             211W / 350W |  19380MiB / 24576MiB |     92%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        Off | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 59%   58C    P2             184W / 350W |  23516MiB / 24576MiB |     36%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 64%   62C    P2             174W / 350W |  23442MiB / 24576MiB |     35%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 3090        Off | 00000000:88:00.0 Off |                  N/A |\n",
      "| 58%   60C    P2             193W / 350W |  12794MiB / 24576MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 3090        Off | 00000000:89:00.0 Off |                  N/A |\n",
      "| 72%   67C    P2             238W / 350W |  12684MiB / 24576MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090        Off | 00000000:B1:00.0 Off |                  N/A |\n",
      "| 66%   65C    P2             249W / 350W |  15710MiB / 24576MiB |     42%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce RTX 3090        Off | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 63%   62C    P2             205W / 350W |  17226MiB / 24576MiB |     32%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqclQCVqjE3u"
   },
   "source": [
    "### Check if nvcc compiler is capable of using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1741280006697,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "sFT0dpBBjNs1",
    "outputId": "86d17425-6ed1-4616-f1da-b5a8fd811fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:23:50_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqGTiMLEbEdx"
   },
   "source": [
    "### `Jacobi example using stream context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0goqSor0VbID"
   },
   "source": [
    "Check <b>[jacobi_pfor.cu](./stf/jacobi_pfor.cu)</b>, compile and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1741281072261,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "ihrvZEJ0Wd5P"
   },
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include stf/jacobi_pfor.cu -o jacobi_pfor -arch=sm_86 -lcuda --generate-line-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8768,
     "status": "ok",
     "timestamp": 1741280047248,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "bwhOuSG9Wd_P",
    "outputId": "d603299a-ea8e-4d92-8984-0f7f6c1887fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1.710080 ms\n"
     ]
    }
   ],
   "source": [
    "!./jacobi_pfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -t nvtx,cuda,cublas --cuda-event-trace=false --force-overwrite=true -o jacobi_pfor_profile ./jacobi_pfor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks in the Stream backend\n",
    "\n",
    "The `stream_ctx` backend utilizes CUDA streams and events to provide synchronization. Each `stream_task` in the `stream_ctx` backend represents a task that is associated with an input CUDA stream. **Asynchronous work can be submitted in the body of the task** using this input stream. Once the `stream_task` completes, all work submitted within the task’s body is assumed to be synchronized with the associated stream.\n",
    "\n",
    "Question: 提交到一个stream_task中的所有work都是异步执行吗？如果这些work间有依赖呢（嵌套依赖） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `AXPY example using graph context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check <b>[01-axpy-cuda_kernel_chain.cu](./stf/01-axpy-cuda_kernel_chain.cu)</b>, compile and run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    context ctx = graph_ctx();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include stf/01-axpy-cuda_kernel_chain.cu -o 01-axpy-cuda_kernel_chain -arch=sm_86 -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./01-axpy-cuda_kernel_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-dfc2.qdstrm'\n",
      "[1/7] [========================100%] 01-axpy-cuda_kernel_chain_profile.nsys-rep\n",
      "[2/7] [========================100%] 01-axpy-cuda_kernel_chain_profile.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                  Name                \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------\n",
      "     88.6        107030958          1  107030958.0  107030958.0  107030958  107030958          0.0  cudaFree                            \n",
      "     10.5         12655920          1   12655920.0   12655920.0   12655920   12655920          0.0  cudaGraphLaunch_v10000              \n",
      "      0.3           413616          3     137872.0       2761.0       1003     409852     235543.2  cudaGraphAddKernelNode_v10000       \n",
      "      0.2           205747          1     205747.0     205747.0     205747     205747          0.0  cudaGraphInstantiateWithFlags_v11040\n",
      "      0.1           178191          2      89095.5      89095.5      19062     159129      99042.3  cudaHostRegister                    \n",
      "      0.1           113661          2      56830.5      56830.5       1260     112401      78588.6  cudaGraphAddMemAllocNode_v11040     \n",
      "      0.1            99864          1      99864.0      99864.0      99864      99864          0.0  cudaDeviceGetDefaultMemPool_v11020  \n",
      "      0.0            33079          1      33079.0      33079.0      33079      33079          0.0  cudaStreamCreate                    \n",
      "      0.0            18882          1      18882.0      18882.0      18882      18882          0.0  cudaGraphExecDestroy_v10000         \n",
      "      0.0            14735          3       4911.7       3715.0       3220       7800       2513.6  cudaGraphAddMemcpyNode_v10000       \n",
      "      0.0            14148          3       4716.0       5072.0        562       8514       3987.9  cudaGraphAddDependencies_v10000     \n",
      "      0.0             9814          1       9814.0       9814.0       9814       9814          0.0  cudaGraphCreate_v10000              \n",
      "      0.0             8943          1       8943.0       8943.0       8943       8943          0.0  cudaGraphDestroy_v10000             \n",
      "      0.0             7634          1       7634.0       7634.0       7634       7634          0.0  cudaStreamDestroy                   \n",
      "      0.0             6648          2       3324.0       3324.0       1128       5520       3105.6  cudaGraphAddMemFreeNode_v11040      \n",
      "      0.0             6278          1       6278.0       6278.0       6278       6278          0.0  cudaStreamSynchronize               \n",
      "      0.0             3999          1       3999.0       3999.0       3999       3999          0.0  cudaGraphAddEmptyNode_v10000        \n",
      "      0.0             1095          1       1095.0       1095.0       1095       1095          0.0  cudaGraphGetNodes_v10000            \n",
      "      0.0              998          1        998.0        998.0        998        998          0.0  cudaGraphGetEdges_v10000            \n",
      "      0.0              569          1        569.0        569.0        569        569          0.0  cuModuleGetLoadingMode              \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain CUDA kernel data.\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain GPU memory data.\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain GPU memory data.\n",
      "Generated:\n",
      "\t/root/stf_exp/01-axpy-cuda_kernel_chain_profile.nsys-rep\n",
      "\t/root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -t nvtx,cuda,cublas --cuda-event-trace=false --force-overwrite=true -o 01-axpy-cuda_kernel_chain_profile ./01-axpy-cuda_kernel_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `AXPY example using stream context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check <b>[01-axpy-cuda_kernel_chain.cu](./stf/01-axpy-cuda_kernel_chain.cu)</b>, compile and run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    context ctx = stream_ctx();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include stf/01-axpy-cuda_kernel_chain.cu -o 01-axpy-cuda_kernel_chain_stream -arch=sm_86 -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./01-axpy-cuda_kernel_chain_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Generating '/tmp/nsys-report-dfc2.qdstrm'\n",
      "[1/7] [========================100%] 01-axpy-cuda_kernel_chain_profile.nsys-rep\n",
      "[2/7] [========================100%] 01-axpy-cuda_kernel_chain_profile.sqlite\n",
      "[3/7] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/7] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                  Name                \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------\n",
      "     88.6        107030958          1  107030958.0  107030958.0  107030958  107030958          0.0  cudaFree                            \n",
      "     10.5         12655920          1   12655920.0   12655920.0   12655920   12655920          0.0  cudaGraphLaunch_v10000              \n",
      "      0.3           413616          3     137872.0       2761.0       1003     409852     235543.2  cudaGraphAddKernelNode_v10000       \n",
      "      0.2           205747          1     205747.0     205747.0     205747     205747          0.0  cudaGraphInstantiateWithFlags_v11040\n",
      "      0.1           178191          2      89095.5      89095.5      19062     159129      99042.3  cudaHostRegister                    \n",
      "      0.1           113661          2      56830.5      56830.5       1260     112401      78588.6  cudaGraphAddMemAllocNode_v11040     \n",
      "      0.1            99864          1      99864.0      99864.0      99864      99864          0.0  cudaDeviceGetDefaultMemPool_v11020  \n",
      "      0.0            33079          1      33079.0      33079.0      33079      33079          0.0  cudaStreamCreate                    \n",
      "      0.0            18882          1      18882.0      18882.0      18882      18882          0.0  cudaGraphExecDestroy_v10000         \n",
      "      0.0            14735          3       4911.7       3715.0       3220       7800       2513.6  cudaGraphAddMemcpyNode_v10000       \n",
      "      0.0            14148          3       4716.0       5072.0        562       8514       3987.9  cudaGraphAddDependencies_v10000     \n",
      "      0.0             9814          1       9814.0       9814.0       9814       9814          0.0  cudaGraphCreate_v10000              \n",
      "      0.0             8943          1       8943.0       8943.0       8943       8943          0.0  cudaGraphDestroy_v10000             \n",
      "      0.0             7634          1       7634.0       7634.0       7634       7634          0.0  cudaStreamDestroy                   \n",
      "      0.0             6648          2       3324.0       3324.0       1128       5520       3105.6  cudaGraphAddMemFreeNode_v11040      \n",
      "      0.0             6278          1       6278.0       6278.0       6278       6278          0.0  cudaStreamSynchronize               \n",
      "      0.0             3999          1       3999.0       3999.0       3999       3999          0.0  cudaGraphAddEmptyNode_v10000        \n",
      "      0.0             1095          1       1095.0       1095.0       1095       1095          0.0  cudaGraphGetNodes_v10000            \n",
      "      0.0              998          1        998.0        998.0        998        998          0.0  cudaGraphGetEdges_v10000            \n",
      "      0.0              569          1        569.0        569.0        569        569          0.0  cuModuleGetLoadingMode              \n",
      "\n",
      "[5/7] Executing 'cuda_gpu_kern_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain CUDA kernel data.\n",
      "[6/7] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain GPU memory data.\n",
      "[7/7] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "SKIPPED: /root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite does not contain GPU memory data.\n",
      "Generated:\n",
      "\t/root/stf_exp/01-axpy-cuda_kernel_chain_profile.nsys-rep\n",
      "\t/root/stf_exp/01-axpy-cuda_kernel_chain_profile.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -t nvtx,cuda,cublas --cuda-event-trace=false --force-overwrite=true -o 01-axpy-cuda_kernel_chain_profile_stream ./01-axpy-cuda_kernel_chain_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tasks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check <b>[axpy-annotated.cu](./stf/axpy-annotated.cu)</b>, compile and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include stf/axpy-annotated.cu -o axpy-annotated -arch=sm_86 -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axpy-annotated: stf/axpy-annotated.cu:80: int main(): Assertion `fabs(Y[i] - (Y0(i) + alpha * X0(i))) < 0.0001' failed.\n"
     ]
    }
   ],
   "source": [
    "!CUDASTF_DOT_FILE=axpy.dot ./axpy-annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! # Generate the visualization from this dot file in PDF or PNG format\n",
    "# ! apt install graphviz\n",
    "! dot -Tpdf axpy.dot -o axpy.pdf\n",
    "! dot -Tpng axpy.dot -o axpy.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check <b>[axpy.pdf](./axpy.pdf)</b>/<b>[axpy.png](./axpy.png)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./axpy.png\" width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Synchronization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are simple explanations for CUDASTF synchronization methods:\n",
    "\n",
    "* `ctx.finalize()` is the main way to wait for all submitted tasks and background operations in the context to complete on the host, while `ctx.submit()` means to initiates the submission of all asynchronous tasks within the sequence.\n",
    "* `ctx.task_fence()` is used to wait for the completion of all pending operations (tasks, transfers, …).\n",
    "```cpp\n",
    "    cudaStream_t stream = ctx.task_fence();\n",
    "    cudaStreamSynchronize(stream);\n",
    "```\n",
    "* `ctx.wait(logical_data)` is a specific blocking method on the host to retrieve the final value of a logical data object, commonly used after reduction tasks.\n",
    "* Inside kernels launched with `ctx.launch`, `th.sync()` and `th.sync(level)` are used as barriers to synchronize threads at specific levels of the thread hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Execution Places`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check <b>[1f1b.cu](./stf/1f1b.cu)</b>, compile and run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    context ctx = stream_ctx();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include stf/1f1b.cu -o 1f1b -arch=sm_86 -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEV 0] cannot enable peer access with device 1\n",
      "[DEV 1] cannot enable peer access with device 0\n",
      "Number of real devices: 2\n",
      "../cccl/cudax/include/cuda/experimental/__stf/stream/interfaces/slice.cuh(241) [device 1] CUDA error in data_copy: invalid argument (cudaErrorInvalidValue).\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 CUDASTF_DOT_FILE=1f1b.dot ./1f1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No peer access support in current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "! # Generate the visualization from this dot file in PDF or PNG format\n",
    "# ! apt install graphviz\n",
    "! dot -Tpdf 1f1b.dot -o 1f1b.pdf\n",
    "! dot -Tpng 1f1b.dot -o 1f1b.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check <b>[1f1b.png](./1f1b.png)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true -t nvtx,cuda,cublas --cuda-event-trace=false --force-overwrite=true -o 1f1b_profile ./1f1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++17 -expt-relaxed-constexpr --extended-lambda -I../cccl/libcudacxx/include -I../cccl/cudax/include stf/axpy-annotated.cu -o axpy-annotated -arch=sm_86 -lcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axpy-annotated: stf/axpy-annotated.cu:80: int main(): Assertion `fabs(Y[i] - (Y0(i) + alpha * X0(i))) < 0.0001' failed.\n"
     ]
    }
   ],
   "source": [
    "!CUDASTF_DOT_FILE=axpy.dot ./axpy-annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! # Generate the visualization from this dot file in PDF or PNG format\n",
    "# ! apt install graphviz\n",
    "! dot -Tpdf axpy.dot -o axpy.pdf\n",
    "! dot -Tpng axpy.dot -o axpy.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "823d_hqHqTdN"
   },
   "source": [
    "### `CUDA Thread Hierarchy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn_S8PRHbcnQ",
    "outputId": "688a243d-44b8-4012-b7ac-aa0f12b79b7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\">\n",
       "<iframe src=\"https://docs.google.com/presentation/d/1J_GF6XACL0-Dk1BtJCeWiHwJCFcM_Hkx/edit?usp=share_link&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
       "\n",
       "</iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\">\n",
    "<iframe src=\"https://docs.google.com/presentation/d/1J_GF6XACL0-Dk1BtJCeWiHwJCFcM_Hkx/edit?usp=share_link&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
    "\n",
    "</iframe></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1741281076893,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "2hTtR30ikC5N",
    "outputId": "3370dd96-897e-480d-a036-f6ec5c63f876",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mm.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile mm.cu\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>\n",
    "\n",
    "__global__ void kernel(int *A, int *B, int size)\n",
    "{\n",
    "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int k;\n",
    "\n",
    "  if((i < size) && (j < size))\n",
    "    for(k = 0; k < size; k++)\n",
    "       B[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "\n",
    "}\n",
    "\n",
    "void initializeMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "    for(j = 0; j < size; j++)\n",
    "       A[i * size + j] = rand() % (10 - 1) * 1;\n",
    "}\n",
    "\n",
    "void printMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "  {\n",
    "    for(j = 0; j < size; j++)\n",
    "       printf(\"%d\\t\", A[i * size + j]);\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int size = atoi(argv[1]);\n",
    "  int blockSize = atoi(argv[2]);\n",
    "  double t1, t2;\n",
    "\n",
    "  // Memory Allocation in the Host\n",
    "  int  *A = (int *) malloc (sizeof(int) * size * size);\n",
    "  int  *B = (int *) malloc (sizeof(int) * size * size);\n",
    "\n",
    "  initializeMatrix(A, size);\n",
    "  initializeMatrix(B, size);\n",
    "\n",
    "  t1 = omp_get_wtime();\n",
    "\n",
    "  // Memory Allocation in the Device\n",
    "  int *d_A, *d_B;\n",
    "  cudaMalloc((void **) &d_A, size * size * sizeof(int));\n",
    "  cudaMalloc((void **) &d_B, size * size * sizeof(int));\n",
    "\n",
    "  // Copy of data from host to device\n",
    "  cudaMemcpy( d_A, A, size * size * sizeof(int), cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy( d_B, B, size * size * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "  // 2D Computational Grid\n",
    "  dim3 dimGrid((int) ceil( (int) size / (int) blockSize ), (int) ceil( (int) size / (int) blockSize ));\n",
    "  dim3 dimBlock( blockSize, blockSize);\n",
    "\n",
    "       kernel<<<dimGrid, dimBlock>>>(A, B, size);\n",
    "\n",
    "  // Copy of data from device to host\n",
    "  cudaMemcpy( B, d_B, size * size * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "  t2 = omp_get_wtime();\n",
    "\n",
    "  printf(\"%d\\t%f\\n\", size, t2-t1);\n",
    "\n",
    " //printMatrix(B, size);\n",
    "\n",
    " // Memory Allocation in the Device\n",
    " cudaFree(d_A);\n",
    " cudaFree(d_B);\n",
    "\n",
    " // Memory Allocation in the Host\n",
    " free(A);\n",
    " free(B);\n",
    "\n",
    " return 0;\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1216,
     "status": "ok",
     "timestamp": 1741281081303,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "FTVBh76aSywY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 mm.cu -o mm -Xcompiler -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1741280070460,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "TCAWmPrRYc6W",
    "outputId": "20184152-0963-4380-9aad-80316fe76cf9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t0.239031\n"
     ]
    }
   ],
   "source": [
    "!./mm 1000 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edaoNwRFeJK9"
   },
   "source": [
    "### `Grid-Stride Loops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymjx2tcvoTYF",
    "outputId": "0eabe683-f0f0-44bd-e470-96349fd2c707"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\">\n",
       "<iframe src=\"https://docs.google.com/presentation/d/1tRO-HwqCfv8imhDO4S_8yAv8wEcJVttZ/edit?usp=sharing&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
       "\n",
       "</iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\">\n",
    "<iframe src=\"https://docs.google.com/presentation/d/1tRO-HwqCfv8imhDO4S_8yAv8wEcJVttZ/edit?usp=sharing&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
    "\n",
    "</iframe></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1741281084755,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "MZpA7sUlLVgZ",
    "outputId": "7bddce47-bfae-42d7-df79-45b7422e8044",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mm-gridStrideLoop.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile mm-gridStrideLoop.cu\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>\n",
    "\n",
    "__global__ void kernel(int *A, int *B, int size)\n",
    "{\n",
    "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int k;\n",
    "\n",
    "  if((i < size) && (j < size))\n",
    "    for(k = 0; k < size; k++)\n",
    "       B[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void kernelGridStrideLoop(int *A, int *B, int size)\n",
    "{\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int idy = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int stride = gridDim.x * blockDim.x;\n",
    "  int i, j, k;\n",
    "\n",
    "  for(i = idx; i < size; i += stride)\n",
    "    for(j = idy; j < size; j += stride)\n",
    "    {\n",
    "       for(k = 0; k < size; k++)\n",
    "            B[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "void initializeMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "    for(j = 0; j < size; j++)\n",
    "      A[i * size + j] = rand() % (10 - 1) * 1;\n",
    "}\n",
    "\n",
    "void printMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "  {\n",
    "    for(j = 0; j < size; j++)\n",
    "      printf(\"%d\\t\", A[i * size + j]);\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int size = atoi(argv[1]);\n",
    "  int blockSize = atoi(argv[2]);\n",
    "  double t1, t2;\n",
    "\n",
    "  // Memory Allocation in the Host\n",
    "  int  *A = (int *) malloc (sizeof(int) * size * size);\n",
    "  int  *B = (int *) malloc (sizeof(int) * size * size);\n",
    "\n",
    "  initializeMatrix(A, size);\n",
    "  initializeMatrix(B, size);\n",
    "\n",
    "  t1 = omp_get_wtime();\n",
    "\n",
    "  // Memory Allocation in the Device\n",
    "  int *d_A, *d_B;\n",
    "  cudaMalloc((void **) &d_A, size * size * sizeof(int));\n",
    "  cudaMalloc((void **) &d_B, size * size * sizeof(int));\n",
    "\n",
    "  // Copy of data from host to device\n",
    "  cudaMemcpy( d_A, A, size * size * sizeof(int), cudaMemcpyHostToDevice );\n",
    "  cudaMemcpy( d_B, B, size * size * sizeof(int), cudaMemcpyHostToDevice );\n",
    "\n",
    "  // 2D Computational Grid\n",
    "  dim3 dimGrid( (int) ceil( (int) size / (int) blockSize ), (int) ceil( (int) size / (int) blockSize ) );\n",
    "  dim3 dimBlock( blockSize, blockSize);\n",
    "\n",
    "            kernelGridStrideLoop<<<dimGrid, dimBlock>>>(A, B, size);\n",
    "\n",
    "  // Copy of data from device to host\n",
    "  cudaMemcpy( B, d_B, size * size * sizeof(int), cudaMemcpyDeviceToHost );\n",
    "\n",
    "  t2 = omp_get_wtime();\n",
    "\n",
    "  printf(\"%d\\t%f\\n\", size, t2-t1);\n",
    "\n",
    " //printMatrix(A, size);\n",
    " //printMatrix(B, size);\n",
    "\n",
    " // Memory Allocation in the Device\n",
    " cudaFree(d_A);\n",
    " cudaFree(d_B);\n",
    "\n",
    " // Memory Allocation in the Host\n",
    " free(A);\n",
    " free(B);\n",
    "\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1213,
     "status": "ok",
     "timestamp": 1741281089603,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "W6r-gx0_Ld2G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 mm-gridStrideLoop.cu -o mm-gridStrideLoop -Xcompiler -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1741280091356,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "RL9LL2OmLhvM",
    "outputId": "eabb45d8-ed05-4955-c819-1f16634b93c6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t0.136449\n"
     ]
    }
   ],
   "source": [
    "!./mm-gridStrideLoop 1000 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFTvpmQBeJK-"
   },
   "source": [
    "### `Unified Memory (cudaMallocManaged)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3ThCItfoTYH",
    "outputId": "9076f4a7-554d-4af0-b062-be5ef926e55f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\">\n",
       "<iframe src=\"https://docs.google.com/presentation/d/1ZgEGCivfxKS6DDHsq1-3-k4YELQQknZ0/edit?usp=share_link&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
       "\n",
       "</iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\">\n",
    "<iframe src=\"https://docs.google.com/presentation/d/1ZgEGCivfxKS6DDHsq1-3-k4YELQQknZ0/edit?usp=share_link&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
    "\n",
    "</iframe></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1741281094816,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "1B3TRZ5CeJK_",
    "outputId": "4b674511-4392-4ecc-bdb6-817d1ffd3efe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mm-cudaMallocManaged.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile mm-cudaMallocManaged.cu\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>\n",
    "\n",
    "__global__ void kernel(int *A, int *B, int size)\n",
    "{\n",
    "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int k;\n",
    "\n",
    "  if((i < size) && (j < size))\n",
    "     for(k = 0; k < size; k++)\n",
    "        B[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void kernelGridStrideLoop(int *A, int *B, int size)\n",
    "{\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int idy = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int stride = gridDim.x * blockDim.x;\n",
    "  int i, j, k;\n",
    "\n",
    "  for(i = idx; i < size; i += stride)\n",
    "    for(j = idy; j < size; j += stride)\n",
    "    {\n",
    "       for(k = 0; k < size; k++)\n",
    "            B[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "void initializeMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "    for(j = 0; j < size; j++)\n",
    "      A[i * size + j] = rand() % (10 - 1) * 1;\n",
    "}\n",
    "\n",
    "void printMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "  {\n",
    "    for(j = 0; j < size; j++)\n",
    "      printf(\"%d\\t\", A[i * size + j]);\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    " int size = atoi(argv[1]);\n",
    " int blockSize = atoi(argv[2]); ;\n",
    " double t1, t2;\n",
    " int *A,  *B;\n",
    "\n",
    " t1 = omp_get_wtime();\n",
    "\n",
    " cudaMallocManaged(&A, sizeof(int) * size * size);\n",
    " cudaMallocManaged(&B, sizeof(int) * size * size);\n",
    "\n",
    " initializeMatrix(A, size);\n",
    " initializeMatrix(B, size);\n",
    "\n",
    " dim3 dimGrid( (int) ceil( (int) size / (int) blockSize ), (int) ceil( (int) size / (int) blockSize ) );\n",
    " dim3 dimBlock( blockSize, blockSize);\n",
    "\n",
    "      kernelGridStrideLoop<<<dimGrid, dimBlock>>>(A, B, size);\n",
    "      cudaDeviceSynchronize();\n",
    "\n",
    " t2 = omp_get_wtime();\n",
    "\n",
    "printf(\"%d\\t%f\\n\", size, (t2-t1));\n",
    "\n",
    "//printMatrix(A, size);\n",
    "//printMatrix(B, size);\n",
    "\n",
    "// Free all our allocated memory\n",
    "cudaFree(A);\n",
    "cudaFree(B);\n",
    "\n",
    "return 0;\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1248,
     "status": "ok",
     "timestamp": 1741281100355,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "inrnf5OpeJK_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc mm-cudaMallocManaged.cu -o mm-cudaMallocManaged -Xcompiler -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1741280114836,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "qfwoEnWKeJK_",
    "outputId": "936d3368-523f-4db8-f7b9-2a0728a56923",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t0.177393\n"
     ]
    }
   ],
   "source": [
    "!./mm-cudaMallocManaged 1000 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYP1lJrZok32"
   },
   "source": [
    "#### `Streaming Multiprocessors (SMs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLa0RujAoTYI",
    "outputId": "4b8e4b5b-f784-4383-9419-e2b8f01a97ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\">\n",
       "<iframe src=\"https://docs.google.com/presentation/d/18z3x55kxCCjGZ3LVKOtSN5q8qXe4swFL/edit?usp=sharing&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
       "\n",
       "</iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\">\n",
    "<iframe src=\"https://docs.google.com/presentation/d/18z3x55kxCCjGZ3LVKOtSN5q8qXe4swFL/edit?usp=sharing&ouid=117965215426975519312&rtpof=true&sd=true\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\">\n",
    "\n",
    "</iframe></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1741281104423,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "IH8IX1MAok33",
    "outputId": "cd540f71-af05-4075-c92e-ee6194e36f80",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mm-streamingMultiprocessors.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile mm-streamingMultiprocessors.cu\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>\n",
    "\n",
    "__global__ void kernelGridStrideLoop(int *A, int *B, int size)\n",
    "{\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int idy = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "  int stride = gridDim.x * blockDim.x;\n",
    "  int i, j, k;\n",
    "\n",
    "  for(i = idx; i < size; i += stride)\n",
    "    for(j = idy; j < size; j += stride)\n",
    "    {\n",
    "       for(k = 0; k < size; k++)\n",
    "         B[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "void initializeMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "    for(j = 0; j < size; j++)\n",
    "      A[i * size + j] = rand() % (10 - 1) * 1;\n",
    "}\n",
    "\n",
    "void printMatrix(int *A, int size)\n",
    "{\n",
    "  int i, j;\n",
    "\n",
    "  for(i = 0; i < size; i++)\n",
    "  {\n",
    "    for(j = 0; j < size; j++)\n",
    "      printf(\"%d\\t\", A[i * size + j]);\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main (int argc, char **argv)\n",
    "{\n",
    " int size = atoi(argv[1]);\n",
    " int sizeblock = atoi(argv[2]); ;\n",
    " double t1, t2;\n",
    " int *A,  *B;\n",
    "\n",
    " t1 = omp_get_wtime();\n",
    "\n",
    " cudaMallocManaged(&A, sizeof(int) * size * size);\n",
    " cudaMallocManaged(&B, sizeof(int) * size * size);\n",
    "\n",
    " initializeMatrix(A, size);\n",
    " initializeMatrix(B, size);\n",
    "\n",
    " int deviceId, numberOfSMs;\n",
    " cudaGetDevice(&deviceId);\n",
    " cudaDeviceGetAttribute(&numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId);\n",
    "\n",
    " int NUMBER_OF_BLOCKS = numberOfSMs * 32;\n",
    " int NUMBER_OF_THREADS = 1024;\n",
    "\n",
    "      kernelGridStrideLoop<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS>>>(A, B, size);\n",
    "      cudaDeviceSynchronize();\n",
    "\n",
    " t2 = omp_get_wtime();\n",
    "\n",
    " printf(\"%d\\t%f\\n\", size, t2-t1);\n",
    "\n",
    "//printMatrix(B, size);\n",
    "\n",
    "// Free all our allocated memory\n",
    " cudaFree(A);\n",
    " cudaFree(B);\n",
    "\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 1218,
     "status": "ok",
     "timestamp": 1741281109424,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "jXKV9XKZok33",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc mm-streamingMultiprocessors.cu -o mm-streamingMultiprocessors -Xcompiler -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1741280131254,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "JAg-VF-qok33",
    "outputId": "48cd54a4-a30b-425f-f9d0-a62b1eea5d48",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t0.174564\n"
     ]
    }
   ],
   "source": [
    "!./mm-streamingMultiprocessors 1000 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mTPpDjT1Ik8"
   },
   "source": [
    "## `Profilling GPU core`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YechZ8am1Ik8"
   },
   "source": [
    "The GPU has many units working in parallel, and it is common for it to be bound by different units at different frame sequences. Due to the nature of this behavior, it is beneficial to identify where the GPU cost is going when searching for bottlenecks and to understand what a GPU bottleneck is. Some applications help developers identify bottlenecks, which is useful when optimizing performance, following some NVIDIA profilling tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1741280844682,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "V4CLWbnJ1Ik8",
    "outputId": "b89c6010-5744-498e-918c-fa79c9208dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vector-add.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile vector-add.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "void initWith(float num, float *a, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; ++i)\n",
    "    a[i] = num;\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void addVectorsInto(float *result, float *a, float *b, int N)\n",
    "{\n",
    "  int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "\n",
    "  for(int i = index; i < N; i += stride)\n",
    "    result[i] = a[i] + b[i];\n",
    "}\n",
    "\n",
    "void checkElementsAre(float target, float *vector, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; i++)\n",
    "  {\n",
    "    if(vector[i] != target)\n",
    "    {\n",
    "      printf(\"FAIL: vector[%d] - %0.0f does not equal %0.0f\\n\", i, vector[i], target);\n",
    "      exit(1);\n",
    "    }\n",
    "  }\n",
    "  printf(\"Success! All values calculated correctly.\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  const int N = 2<<24;\n",
    "  size_t size = N * sizeof(float);\n",
    "\n",
    "  float *a;\n",
    "  float *b;\n",
    "  float *c;\n",
    "\n",
    "  cudaMallocManaged(&a, size);\n",
    "  cudaMallocManaged(&b, size);\n",
    "  cudaMallocManaged(&c, size);\n",
    "\n",
    "  initWith(3, a, N);\n",
    "  initWith(4, b, N);\n",
    "  initWith(0, c, N);\n",
    "\n",
    "  size_t threadsPerBlock;\n",
    "  size_t numberOfBlocks;\n",
    "\n",
    "  int deviceId;\n",
    "  cudaGetDevice(&deviceId);\n",
    "\n",
    "  cudaDeviceProp props;\n",
    "  cudaGetDeviceProperties(&props, deviceId);\n",
    "  int multiProcessorCount = props.multiProcessorCount;\n",
    "  threadsPerBlock = 1024;\n",
    "  numberOfBlocks = 32 * multiProcessorCount;\n",
    "\n",
    "  cudaError_t addVectorsErr;\n",
    "  cudaError_t asyncErr;\n",
    "\n",
    "  addVectorsInto<<<numberOfBlocks, threadsPerBlock>>>(c, a, b, N);\n",
    "\n",
    "  addVectorsErr = cudaGetLastError();\n",
    "  if(addVectorsErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(addVectorsErr));\n",
    "\n",
    "  asyncErr = cudaDeviceSynchronize();\n",
    "  if(asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
    "\n",
    "  checkElementsAre(7, c, N);\n",
    "\n",
    "  // Free all our allocated memory\n",
    "  cudaFree(a);\n",
    "  cudaFree(b);\n",
    "  cudaFree(c);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1303,
     "status": "ok",
     "timestamp": 1741280937444,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "UpiUEyPE1Ik9"
   },
   "outputs": [],
   "source": [
    "!nvcc vector-add.cu -o vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXmO9bX01Ik9"
   },
   "source": [
    "### ⊗ NSYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwZ2MM3A1Ik9"
   },
   "source": [
    "`NVIDIA Nsight Systems` (nsys) is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZF7baCM1Ik9"
   },
   "source": [
    "The command `nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the nsys im command line. In the next, you will be using the generated report files to give to the Nsight Systems GUI for visual profilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2729,
     "status": "ok",
     "timestamp": 1741280943127,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "qrq38G5Y1Ik-",
    "outputId": "af63c5f4-640d-4bec-fe21-0afd14b81d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Error: the provided PTX was compiled with an unsupported toolchain.\n",
      "FAIL: vector[0] - 0 does not equal 7\n",
      "Generating '/tmp/nsys-report-150b.qdstrm'\n",
      "[1/8] [========================100%] report1.nsys-rep\n",
      "[2/8] [========================100%] report1.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /content/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
      " --------  ---------------  ---------  ------------  ------------  --------  -----------  ------------  ----------------------\n",
      "     95.9    1,220,708,427         56  21,798,364.8  10,077,186.5     1,245  316,726,757  47,004,542.7  poll                  \n",
      "      3.8       48,384,557        490      98,744.0      11,310.5     1,035   16,905,786     828,425.6  ioctl                 \n",
      "      0.2        2,008,964         27      74,406.1      14,791.0    10,641    1,188,335     224,403.9  mmap64                \n",
      "      0.0          398,525          9      44,280.6      34,350.0    22,189      100,612      23,865.4  sem_timedwait         \n",
      "      0.0          386,521         45       8,589.4       7,517.0     2,548       36,184       5,165.1  open64                \n",
      "      0.0          345,880         50       6,917.6       3,508.0     1,392       59,585       9,937.7  fopen                 \n",
      "      0.0          237,829         20      11,891.5       7,491.5     2,349       72,573      15,335.4  mmap                  \n",
      "      0.0          133,282          2      66,641.0      66,641.0    63,813       69,469       3,999.4  pthread_create        \n",
      "      0.0           95,918          1      95,918.0      95,918.0    95,918       95,918           0.0  pthread_cond_wait     \n",
      "      0.0           94,281          9      10,475.7       8,170.0     4,304       25,286       6,370.6  munmap                \n",
      "      0.0           82,278         41       2,006.8       1,589.0     1,045        7,835       1,240.2  fclose                \n",
      "      0.0           65,150         10       6,515.0       6,965.5     3,881        8,311       1,426.1  write                 \n",
      "      0.0           43,190          1      43,190.0      43,190.0    43,190       43,190           0.0  fgets                 \n",
      "      0.0           32,740          6       5,456.7       5,602.0     2,073        8,216       2,120.5  open                  \n",
      "      0.0           18,886          2       9,443.0       9,443.0     6,743       12,143       3,818.4  socket                \n",
      "      0.0           16,532          3       5,510.7       6,538.0     2,971        7,023       2,212.7  pipe2                 \n",
      "      0.0           13,717          7       1,959.6       1,997.0     1,274        2,832         609.5  read                  \n",
      "      0.0           12,342          1      12,342.0      12,342.0    12,342       12,342           0.0  fopen64               \n",
      "      0.0           10,378          2       5,189.0       5,189.0     4,211        6,167       1,383.1  pthread_cond_broadcast\n",
      "      0.0            9,320          1       9,320.0       9,320.0     9,320        9,320           0.0  connect               \n",
      "      0.0            7,815          3       2,605.0       1,157.0     1,127        5,531       2,534.0  fcntl                 \n",
      "      0.0            6,255          2       3,127.5       3,127.5     2,220        4,035       1,283.4  fwrite                \n",
      "      0.0            3,358          1       3,358.0       3,358.0     3,358        3,358           0.0  fread                 \n",
      "      0.0            1,903          1       1,903.0       1,903.0     1,903        1,903           0.0  getentropy            \n",
      "      0.0            1,474          1       1,474.0       1,474.0     1,474        1,474           0.0  bind                  \n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)    StdDev (ns)                 Name               \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  -----------  ------------  ---------------------------------\n",
      "     70.3      129,623,763          3  43,207,921.0      41,481.0      29,023  129,553,259  74,777,256.5  cudaMallocManaged                \n",
      "     29.6       54,583,250          1  54,583,250.0  54,583,250.0  54,583,250   54,583,250           0.0  cudaLaunchKernel                 \n",
      "      0.1          175,747          1     175,747.0     175,747.0     175,747      175,747           0.0  cudaGetDeviceProperties_v2_v12000\n",
      "      0.0           13,640          1      13,640.0      13,640.0      13,640       13,640           0.0  cudaDeviceSynchronize            \n",
      "      0.0              885          1         885.0         885.0         885          885           0.0  cuModuleGetLoadingMode           \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "SKIPPED: /content/report1.sqlite does not contain CUDA kernel data.\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "SKIPPED: /content/report1.sqlite does not contain GPU memory data.\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "SKIPPED: /content/report1.sqlite does not contain GPU memory data.\n",
      "Generated:\n",
      "    /content/report1.nsys-rep\n",
      "    /content/report1.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT64sX-L1Ik-"
   },
   "source": [
    "After profilling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profilling output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtrU5O6S1Ik_"
   },
   "source": [
    "## `Concurrent CUDA Streams`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUweGxxw1Ik_"
   },
   "source": [
    "In CUDA programming, a **stream** is a series of commands that execute in order. In CUDA applications, kernel execution, as well as some memory transfers, occur within CUDA streams. Up until this point in time, you have not been interacting explicitly with CUDA streams, but in fact, your CUDA code has been executing its kernels inside of a stream called *the default stream*. CUDA programmers can create and utilize non-default CUDA streams in addition to the default stream, and in doing so, perform multiple operations, such as executing multiple kernels, concurrently, in different streams. Using multiple streams can add an additional layer of parallelization to your accelerated applications, and offers many more opportunities for application optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg4zrFGk1Ik_"
   },
   "source": [
    "### Creating, Utilizing, and Destroying Non-Default CUDA Streams\n",
    "\n",
    "The following code snippet demonstrates how to create, utilize, and destroy a non-default CUDA stream. You will note, that to launch a CUDA kernel in a non-default CUDA stream, the stream must be passed as the optional 4th argument of the execution configuration. Up until now you have only utilized the first 2 arguments of the execution configuration:\n",
    "\n",
    "```cpp\n",
    "cudaStream_t stream;       // CUDA streams are of type `cudaStream_t`.\n",
    "cudaStreamCreate(&stream); // Note that a pointer must be passed to `cudaCreateStream`.\n",
    "\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>(); // `stream` is passed as 4th EC argument.\n",
    "\n",
    "cudaStreamDestroy(stream); // Note that a value, not a pointer, is passed to `cudaDestroyStream`.\n",
    "```\n",
    "\n",
    "Outside the scope of this lab, but worth mentioning, is the optional 3rd argument of the execution configuration. This argument allows programmers to supply the number of bytes in **shared memory** (an advanced topic that will not be covered presently) to be dynamically allocated per block for this kernel launch. The default number of bytes allocated to shared memory per block is `0`, and for the remainder of the lab, you will be passing `0` as this value, in order to expose the 4th argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1741281119282,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "3ytIvT6p1Ik_",
    "outputId": "eec3db61-108b-4db8-a7d1-6ef82555596d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing print-numbers-solution.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile print-numbers-solution.cu\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "__global__ void printNumber(int number)\n",
    "{\n",
    "  printf(\"%d\\n\", number);\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int i;\n",
    "\n",
    "  for(i = 0; i < 5; ++i)\n",
    "  {\n",
    "    cudaStream_t stream;\n",
    "    cudaStreamCreate(&stream);\n",
    "    printNumber<<<1, 1, 0, stream>>>(i);\n",
    "    cudaStreamDestroy(stream);\n",
    "  }\n",
    "\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1741281123498,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "T6W2VoQQ1Ik_"
   },
   "outputs": [],
   "source": [
    "!nvcc print-numbers-solution.cu -o print-numbers-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N68wziuG1Ik_",
    "outputId": "741030ae-ef47-4917-b015-c8409cca3ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "!./print-numbers-solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVPoo5Vt1Ik_"
   },
   "source": [
    "## `Asynchronous Memory Prefetching`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H5Fx3zT1Ik_"
   },
   "source": [
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1741281127983,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "lLrBwHKY1Ik_",
    "outputId": "c460d963-7b84-462b-b558-c591b542bcf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vector-add-prefetching.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile vector-add-prefetching.cu\n",
    "#include <stdio.h>\n",
    "#define N 2048 * 2048 // Number of elements in each vector\n",
    "\n",
    "__global__ void saxpy(int * a, int * b, int * c)\n",
    "{\n",
    "  // Determine our unique global thread ID, so we know which element to process\n",
    "  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = blockDim.x * gridDim.x;\n",
    "\n",
    "  for (int i = tid; i < N; i += stride)\n",
    "     c[i] = 2 * a[i] + b[i];\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int *a, *b, *c;\n",
    "\n",
    "  int size = N * sizeof (int); // The total number of bytes per vector\n",
    "\n",
    "  int deviceId;\n",
    "  int numberOfSMs;\n",
    "\n",
    "  cudaGetDevice(&deviceId);\n",
    "  cudaDeviceGetAttribute(&numberOfSMs, cudaDevAttrMultiProcessorCount, deviceId);\n",
    "\n",
    "  // Allocate memory\n",
    "  cudaMallocManaged(&a, size);\n",
    "  cudaMallocManaged(&b, size);\n",
    "  cudaMallocManaged(&c, size);\n",
    "\n",
    "  // Initialize memory\n",
    "  for(int i = 0; i < N; ++i )\n",
    "  {\n",
    "    a[i] = 2;\n",
    "    b[i] = 1;\n",
    "    c[i] = 0;\n",
    "  }\n",
    "\n",
    "  cudaMemPrefetchAsync(a, size, deviceId);\n",
    "  cudaMemPrefetchAsync(b, size, deviceId);\n",
    "  cudaMemPrefetchAsync(c, size, deviceId);\n",
    "\n",
    "  int threads_per_block = 256;\n",
    "  int number_of_blocks = numberOfSMs * 32;\n",
    "\n",
    "  saxpy <<<number_of_blocks, threads_per_block>>>( a, b, c );\n",
    "\n",
    "  cudaDeviceSynchronize(); // Wait for the GPU to finish\n",
    "\n",
    "  // Print out the first and last 5 values of c for a quality check\n",
    "  for( int i = 0; i < 5; ++i )\n",
    "    printf(\"c[%d] = %d, \", i, c[i]);\n",
    "  printf (\"\\n\");\n",
    "  for( int i = N-5; i < N; ++i )\n",
    "    printf(\"c[%d] = %d, \", i, c[i]);\n",
    "  printf (\"\\n\");\n",
    "\n",
    "  // Free all our allocated memory\n",
    "  cudaFree(a);\n",
    "  cudaFree(b);\n",
    "  cudaFree(c);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1741281133053,
     "user": {
      "displayName": "Murilo Boratto",
      "userId": "13998345321799991370"
     },
     "user_tz": 180
    },
    "id": "HMnTi8Bf1IlA"
   },
   "outputs": [],
   "source": [
    "!nvcc vector-add-prefetching.cu -o vector-add-prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ivpi6Npj1IlA",
    "outputId": "3d14beea-099f-4a92-b7c3-bfd82908dfad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n"
     ]
    }
   ],
   "source": [
    "!./vector-add-prefetching"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1MCx31lj9AcWrqGReFyiNTFoHwErZ2iC5",
     "timestamp": 1739366102672
    },
    {
     "file_id": "1iD5CokXvx02AYYVFCBCpmeNRvtmMgIvJ",
     "timestamp": 1705313939307
    },
    {
     "file_id": "1ZkyYkRzX6g3uldQhU-bgMF5jzn1k3C7J",
     "timestamp": 1705310081089
    },
    {
     "file_id": "19ofstzfOeJlZYKFSrJk6seqe2oJWv1xS",
     "timestamp": 1698257221671
    },
    {
     "file_id": "1OhujvtZ8HLCXG4JU56I72vBJlfqYb9Kk",
     "timestamp": 1694022921959
    },
    {
     "file_id": "1e_lXiysoxz98IAygtljP-0xTOO6FaYV4",
     "timestamp": 1692812241795
    },
    {
     "file_id": "https://github.com/muriloboratto/programacao-jogos-digitais/blob/master/JD0007/notebooks/01-Entradas-Saidas.ipynb",
     "timestamp": 1678191303998
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
